{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaNVFIQZ-HLL"
   },
   "source": [
    "Copyright 2018 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    " [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6mXxhKedaRW"
   },
   "source": [
    "# Conditional Neural Processes (CNP) for 1D regression.\n",
    "\n",
    "[Conditional Neural Processes](https://arxiv.org/pdf/1807.01613.pdf) (CNPs)Â were\n",
    "introduced as a continuation of\n",
    "[Generative Query Networks](https://deepmind.com/blog/neural-scene-representation-and-rendering/)\n",
    "(GQN) to extend its training regime to tasks beyond scene rendering, e.g. to\n",
    "regression and classification.\n",
    "\n",
    "In contrast to most standard neural networks, CNPs learn to approximate a\n",
    "distribution over functions rather than approximating just a single function. As a result, at\n",
    "test time CNPs are flexible and can approximate any function from this\n",
    "distribution when provided with a handful of observations. In addition, they\n",
    "learn to estimate the uncertainty of their prediction from the dataset and as\n",
    "the number of observations is increased this uncertainty reduces and the\n",
    "accuracy of their prediction increases.\n",
    "\n",
    "In this notebook we describe the different parts of a CNP and apply the\n",
    "resulting model to a 1D regression task where a CNP is trained on a dataset of\n",
    "random functions.\n",
    "\n",
    "Any thoughts or questions? We'd love any feedback (about this notebook or CNPs\n",
    "in general) so just contact us at garnelo@google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1nCi4-vkpdM"
   },
   "source": [
    "## Implementing CNPs\n",
    "\n",
    "We start by importing the necessary dependencies. We will make use of numpy,\n",
    "tensorflow, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "enLH-GEtVr_J"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0-rc0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDIziZYOLTgX"
   },
   "source": [
    "## Training data\n",
    "\n",
    "A crucial property of CNPs is their flexibility at test time, as they can model\n",
    "a whole range of functions and narrow down their prediction as we condition on\n",
    "an increasing number of context observations. This behaviour is a result of the\n",
    "training regime of CNPs which is reflected in our datasets.\n",
    "\n",
    "![](https://bit.ly/2O2Lq8c)\n",
    "\n",
    "Rather than training using observations from a single function as it is often\n",
    "the case in machine learning (for example value functions in reinforcement\n",
    "learning) we will use a dataset that consists of many different functions that\n",
    "share some underlying characteristics. This is visualized in the figure above.\n",
    "The example on the left corresponds to a classic training regime: we have a\n",
    "single underlying ground truth function (eg. our value function for an agent) in\n",
    "grey and at each learning iteration we are provided with a handful of examples from this\n",
    "function that we have visualized in different colours for batches of different\n",
    "iterations. On the right we show an example of a dataset that could be used for\n",
    "training neural processes. Instead of a single function, it consists of a large number of functions of a function-class that we are interested in modeling. At each iteration we randomly choose one from the dataset and provide some observations from that function for training. For the next iteration we put that function back and\n",
    "pick a new one from our dataset and use this new function to select the training\n",
    "data. This type of dataset ensures that our model can't overfit to a single\n",
    "function but rather learns a distribution over functions. This idea of a\n",
    "hierarchical dataset also lies at the core of current meta-learning methods.\n",
    "Examples of such datasets could be:\n",
    "\n",
    "*  Functions describing the evolution of temperature over time in different cities \n",
    "of the world.\n",
    "*  A dataset of functions generated by a motion capture sensor of different humans\n",
    "    walking.\n",
    "*   As in this particular example differents functions generated by a Gaussian process (GP)\n",
    "    with a specific kernel.\n",
    "\n",
    "We have chosen GPs for the data generation of this example because they\n",
    "constitute an easy way of sampling smooth curves that share some underlying\n",
    "characteristic (in this case the kernel). Other than for data generation of this\n",
    "particular example neural processes do not make use of kernels or GPs as they\n",
    "are implemented as neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBp9NkWGw-n_"
   },
   "source": [
    "## Data generator\n",
    "\n",
    "In the following section we provide the code for generating our training and\n",
    "testing sets using a GP to generate a dataset of functions. As we will explain\n",
    "later, CNPs use two subset of points at every iteration: one to serve as the\n",
    "context, and the other as targets. In practise we found that including the\n",
    "context points as targets together with some additional new points helped during\n",
    "training. Our data generator divides the generated data into these two groups\n",
    "and provides it in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "SI188jyyJvHl"
   },
   "outputs": [],
   "source": [
    "# The CNP takes as input a `CNPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tesor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "CNPRegressionDescription = collections.namedtuple(\n",
    "    \"CNPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.4,\n",
    "               sigma_scale=1.0,\n",
    "               testing=False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor with shape\n",
    "      `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "    \"\"\"\n",
    "    num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    norm = tf.reduce_sum(\n",
    "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points]\n",
    "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `CNPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    num_context = tf.random.uniform(\n",
    "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
    "\n",
    "    # If we are testing we want to have more targets and have them evenly\n",
    "    # distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      x_values = tf.tile(\n",
    "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
    "          [self._batch_size, 1])\n",
    "      x_values = tf.expand_dims(x_values, axis=-1)\n",
    "    # During training the number of target points and their x-positions are\n",
    "    # selected at random\n",
    "    else:\n",
    "      num_target = tf.random.uniform(\n",
    "          shape=(), minval=2, maxval=self._max_num_context, dtype=tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      x_values = tf.random.uniform(\n",
    "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    # Set kernel parameters\n",
    "    l1 = (\n",
    "        tf.ones(shape=[self._batch_size, self._y_size, self._x_size]) *\n",
    "        self._l1_scale)\n",
    "    sigma_f = tf.ones(\n",
    "        shape=[self._batch_size, self._y_size]) * self._sigma_scale\n",
    "\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    cholesky = tf.cast(tf.linalg.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve\n",
    "    # [batch_size, y_size, num_total_points, 1]\n",
    "    y_values = tf.matmul(\n",
    "        cholesky,\n",
    "        tf.random.normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # [batch_size, num_total_points, y_size]\n",
    "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations\n",
    "      idx = tf.random.shuffle(tf.range(num_target))\n",
    "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as\n",
    "      # some new target points\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "\n",
    "    query = ((context_x, context_y), target_x)\n",
    "\n",
    "    return CNPRegressionDescription(\n",
    "        query=query,\n",
    "        target_y=target_y,\n",
    "        num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqK9Xco1fuNz"
   },
   "source": [
    "## Conditional Neural Processes\n",
    "\n",
    "We can visualise a forward pass in a CNP as follows:\n",
    "\n",
    "<img src=\"https://bit.ly/2OFb6ZK\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "As shown in the diagram, CNPs take in pairs **(x, y)<sub>i</sub>** of context\n",
    "points, pass them through an **encoder** to obtain\n",
    "individual representations **r<sub>i</sub>** which are combined using an **aggregator**. The resulting representation **r**\n",
    "is then combined with the locations of the targets **x<sub>T</sub>** and passed\n",
    "through a **decoder** that returns a mean estimate\n",
    "of the **y** value at that target location together with a measure of the\n",
    "uncertainty over said prediction. Implementing CNPs therefore involves coding up\n",
    "the three main building blocks:\n",
    "\n",
    "*   Encoder\n",
    "*   Aggregator\n",
    "*   Decoder\n",
    "\n",
    "A more detailed description of these three parts is presented in the following\n",
    "sections alongside the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxl8wMT_Io5B"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder **e** is shared between all the context points and consists of an\n",
    "MLP with a handful of layers. For this experiment four layers are enough, but we\n",
    "can still change the number and size of the layers when we build the graph later\n",
    "on via the variable **`encoder_output_sizes`**. Each of the context pairs **(x,\n",
    "y)<sub>i</sub>** results in an individual representation **r<sub>i</sub>** after\n",
    "encoding. These representations are then combined across context points to form\n",
    "a single representation **r** using the aggregator **a**.\n",
    "\n",
    "In this implementation we have included the aggregator **a** in the encoder as\n",
    "we are only taking the mean across all points. The representation **r** produced\n",
    "by the aggregator contains the information about the underlying unknown function\n",
    "**f** that is provided by all the context points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "jw0VYpkQWGhq"
   },
   "outputs": [],
   "source": [
    "class DeterministicEncoder(object):\n",
    "  \"\"\"The Encoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes):\n",
    "    \"\"\"CNP encoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "    \"\"\"\n",
    "    self._output_sizes = output_sizes\n",
    "\n",
    "  def __call__(self, context_x, context_y, num_context_points):\n",
    "    \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "    Args:\n",
    "      context_x: Tensor of size bs x observations x m_ch. For this 1D regression\n",
    "          task this corresponds to the x-values.\n",
    "      context_y: Tensor of size bs x observations x d_ch. For this 1D regression\n",
    "          task this corresponds to the y-values.\n",
    "      num_context_points: A tensor containing a single scalar that indicates the\n",
    "          number of context_points provided in this iteration.\n",
    "\n",
    "    Returns:\n",
    "      representation: The encoded representation averaged over all context \n",
    "          points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate x and y along the filter axes\n",
    "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
    "\n",
    "    # Get the shapes of the input and reshape to parallelise across observations\n",
    "    batch_size, _, filter_size = encoder_input.shape.as_list()\n",
    "    hidden = tf.reshape(encoder_input, (batch_size * num_context_points, -1))\n",
    "    hidden.set_shape((None, filter_size))\n",
    "\n",
    "    # Pass through MLP\n",
    "    # , reuse=tf.compact.v1.AUTO_REUSE\n",
    "    with tf.compat.v1.variable_scope(\"encoder\"):\n",
    "      for i, size in enumerate(self._output_sizes[:-1]):\n",
    "        hidden = tf.nn.relu(\n",
    "            tf.compat.v1.layers.dense(hidden, size, name=\"Encoder_layer_{}\".format(i)))\n",
    "\n",
    "      # Last layer without a ReLu\n",
    "      hidden = tf.compat.v1.layers.dense(\n",
    "          hidden, self._output_sizes[-1], name=\"Encoder_layer_{}\".format(i + 1))\n",
    "\n",
    "    # Bring back into original shape\n",
    "    hidden = tf.reshape(hidden, (batch_size, num_context_points, size))\n",
    "\n",
    "    # Aggregator: take the mean over all points\n",
    "    representation = tf.reduce_mean(hidden, axis=1)\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE7hj8dnz5X2"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "Once we have obtained our representation **r** we concatenate it with each of\n",
    "the targets **x<sub>t</sub>** and pass it through the decoder **d**. As with the\n",
    "encoder **e**, the decoder **d** is shared between all the target points and\n",
    "consists of a small MLP with layer sizes defined in **`decoder_output_sizes`**.\n",
    "The decoder outputs a mean **&mu;<sub>t</sub>** and a variance\n",
    "**&sigma;<sub>t</sub>** for each of the targets **x<sub>t</sub>**. To train our\n",
    "CNP we use the log likelihood of the ground truth value **y<sub>t</sub>** under\n",
    "a Gaussian parametrized by these predicted **&mu;<sub>t</sub>** and\n",
    "**&sigma;<sub>t</sub>**.\n",
    "\n",
    "In this implementation we clip the variance **&sigma;<sub>t</sub>** at 0.1 to\n",
    "avoid collapsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "WGzI381UV7FJ"
   },
   "outputs": [],
   "source": [
    "class DeterministicDecoder(object):\n",
    "  \"\"\"The Decoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes):\n",
    "    \"\"\"CNP decoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
    "          as defined in `basic.Linear`.\n",
    "    \"\"\"\n",
    "    self._output_sizes = output_sizes\n",
    "\n",
    "  def __call__(self, representation, target_x, num_total_points):\n",
    "    \"\"\"Decodes the individual targets.\n",
    "\n",
    "    Args:\n",
    "      representation: The encoded representation of the context\n",
    "      target_x: The x locations for the target query\n",
    "      num_total_points: The number of target points.\n",
    "\n",
    "    Returns:\n",
    "      dist: A multivariate Gaussian over the target points.\n",
    "      mu: The mean of the multivariate Gaussian.\n",
    "      sigma: The standard deviation of the multivariate Gaussian.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate the representation and the target_x\n",
    "    representation = tf.tile(\n",
    "        tf.expand_dims(representation, axis=1), [1, num_total_points, 1])\n",
    "    input = tf.concat([representation, target_x], axis=-1)\n",
    "\n",
    "    # Get the shapes of the input and reshape to parallelise across observations\n",
    "    batch_size, _, filter_size = input.shape.as_list()\n",
    "    hidden = tf.reshape(input, (batch_size * num_total_points, -1))\n",
    "    hidden.set_shape((None, filter_size))\n",
    "\n",
    "    # Pass through MLP\n",
    "    with tf.compat.v1.variable_scope(\"decoder\"):\n",
    "      for i, size in enumerate(self._output_sizes[:-1]):\n",
    "        hidden = tf.nn.relu(\n",
    "            tf.compat.v1.layers.dense(hidden, size, name=\"Decoder_layer_{}\".format(i)))\n",
    "\n",
    "      # Last layer without a ReLu\n",
    "      hidden = tf.compat.v1.layers.dense(\n",
    "          hidden, self._output_sizes[-1], name=\"Decoder_layer_{}\".format(i + 1))\n",
    "\n",
    "    # Bring back into original shape\n",
    "    hidden = tf.reshape(hidden, (batch_size, num_total_points, -1))\n",
    "\n",
    "    # Get the mean an the variance\n",
    "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
    "\n",
    "    # Bound the variance\n",
    "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
    "\n",
    "    # Get the distribution\n",
    "#     dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
    "#         loc=mu, scale_diag=sigma)\n",
    "#     dist = tf.compat.v1.distributions.MultivariateNormalDiag(\n",
    "#         loc=mu, scale_diag=sigma)\n",
    "    dist = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=mu, scale_diag=sigma)\n",
    "    \n",
    "\n",
    "    return dist, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfMjcMHf019M"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now that the main building blocks (encoder, aggregator and decoder) of the CNP\n",
    "are defined we can put everything together into one model. Fundamentally this\n",
    "model only needs to include two main methods: 1. A method that returns the log\n",
    "likelihood of the targets' ground truth values under the predicted\n",
    "distribution.This method will be called during training as our loss function. 2.\n",
    "Another method that returns the predicted mean and variance at the target\n",
    "locations in order to evaluate or query the CNP at test time. This second method\n",
    "needs to be defined separately as, unlike the method above, it should not depend\n",
    "on the ground truth target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "P3LJYP1Qh-jO"
   },
   "outputs": [],
   "source": [
    "class DeterministicModel(object):\n",
    "  \"\"\"The CNP model.\"\"\"\n",
    "\n",
    "  def __init__(self, encoder_output_sizes, decoder_output_sizes):\n",
    "    \"\"\"Initialises the model.\n",
    "\n",
    "    Args:\n",
    "      encoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "          the encoder. The last one is the size of the representation r.\n",
    "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "          the decoder. The last element should correspond to the dimension of\n",
    "          the y * 2 (it encodes both mean and variance concatenated)\n",
    "    \"\"\"\n",
    "    self._encoder = DeterministicEncoder(encoder_output_sizes)\n",
    "    self._decoder = DeterministicDecoder(decoder_output_sizes)\n",
    "\n",
    "  def __call__(self, query, num_total_points, num_contexts, target_y=None):\n",
    "    \"\"\"Returns the predicted mean and variance at the target points.\n",
    "\n",
    "    Args:\n",
    "      query: Array containing ((context_x, context_y), target_x) where:\n",
    "          context_x: Array of shape batch_size x num_context x 1 contains the \n",
    "              x values of the context points.\n",
    "          context_y: Array of shape batch_size x num_context x 1 contains the \n",
    "              y values of the context points.\n",
    "          target_x: Array of shape batch_size x num_target x 1 contains the\n",
    "              x values of the target points.\n",
    "      target_y: The ground truth y values of the target y. An array of \n",
    "          shape batchsize x num_targets x 1.\n",
    "      num_total_points: Number of target points.\n",
    "\n",
    "    Returns:\n",
    "      log_p: The log_probability of the target_y given the predicted\n",
    "      distribution.\n",
    "      mu: The mean of the predicted distribution.\n",
    "      sigma: The variance of the predicted distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    (context_x, context_y), target_x = query\n",
    "\n",
    "    # Pass query through the encoder and the decoder\n",
    "    representation = self._encoder(context_x, context_y, num_contexts)\n",
    "    dist, mu, sigma = self._decoder(representation, target_x, num_total_points)\n",
    "\n",
    "    # If we want to calculate the log_prob for training we will make use of the\n",
    "    # target_y. At test time the target_y is not available so we return None\n",
    "    if target_y is not None:\n",
    "      log_p = dist.log_prob(target_y)\n",
    "    else:\n",
    "      log_p = None\n",
    "\n",
    "    return log_p, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eaNM3-HWqkW"
   },
   "source": [
    "## Plotting function\n",
    "We define a helper function for plotting the intermediate predictions\n",
    "every `PLOT_AFTER` iterations. The ground truth curve will be shown as a black\n",
    "dotted line and the context points from this curve that are fed into the model\n",
    "as black dots. The model's predicted mean and variance is shown in blue for a\n",
    "range of target points in the interval [-2, 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NunDvti3VVwL"
   },
   "outputs": [],
   "source": [
    "def plot_functions(target_x, target_y, context_x, context_y, pred_y, var):\n",
    "  \"\"\"Plots the predicted mean and variance and the context points.\n",
    "  \n",
    "  Args: \n",
    "    target_x: An array of shape batchsize x number_targets x 1 that contains the\n",
    "        x values of the target points.\n",
    "    target_y: An array of shape batchsize x number_targets x 1 that contains the\n",
    "        y values of the target points.\n",
    "    context_x: An array of shape batchsize x number_context x 1 that contains \n",
    "        the x values of the context points.\n",
    "    context_y: An array of shape batchsize x number_context x 1 that contains \n",
    "        the y values of the context points.\n",
    "    pred_y: An array of shape batchsize x number_targets x 1  that contains the\n",
    "        predicted means of the y values at the target points in target_x.\n",
    "    pred_y: An array of shape batchsize x number_targets x 1  that contains the\n",
    "        predicted variance of the y values at the target points in target_x.\n",
    "  \"\"\"\n",
    "  # Plot everything\n",
    "  plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
    "  plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
    "  plt.plot(context_x[0], context_y[0], 'ko', markersize=10)\n",
    "  plt.fill_between(\n",
    "      target_x[0, :, 0],\n",
    "      pred_y[0, :, 0] - var[0, :, 0],\n",
    "      pred_y[0, :, 0] + var[0, :, 0],\n",
    "      alpha=0.2,\n",
    "      facecolor='#65c9f7',\n",
    "      interpolate=True)\n",
    "\n",
    "  # Make the plot pretty\n",
    "  plt.yticks([-2, 0, 2], fontsize=16)\n",
    "  plt.xticks([-2, 0, 2], fontsize=16)\n",
    "  plt.ylim([-2, 2])\n",
    "  plt.grid('off')\n",
    "  ax = plt.gca()\n",
    "  ax.set_axis_bgcolor('white')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFL6HWKoOpgw"
   },
   "source": [
    "## Running Conditional Neural Processes\n",
    "\n",
    "Now that we have defined the dataset as well as our model and its components we\n",
    "can start building everything into the graph. Before we get started we need to\n",
    "set some variables:\n",
    "\n",
    "*   **`TRAINING_ITERATIONS`** - a scalar that describes the number of iterations\n",
    "    for training. At each iteration we will sample a new batch of functions from\n",
    "    the GP, pick some of the points on the curves as our context points **(x,\n",
    "    y)<sub>C</sub>** and some points as our target points **(x,\n",
    "    y)<sub>T</sub>**. We will predict the mean and variance at the target points\n",
    "    given the context and use the log likelihood of the ground truth targets as\n",
    "    our loss to update the model.\n",
    "*   **`MAX_CONTEXT_POINTS`** - a scalar that sets the maximum number of contest\n",
    "    points used during training. The number of context points will then be a\n",
    "    value between 3 and `MAX_CONTEXT_POINTS` that is sampled at random for every\n",
    "    iteration.\n",
    "*   **`PLOT_AFTER`** - a scalar that regulates how often we plot the\n",
    "    intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qSrrfzpPMPz"
   },
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = int(2e5)\n",
    "MAX_CONTEXT_POINTS = 10\n",
    "PLOT_AFTER = int(2e4)\n",
    "# tf.reset_default_graph()\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK6Qb3ZVQFke"
   },
   "source": [
    "We add the dataset reader to the graph for both the training and the testing\n",
    "set. As mentioned above for this experiment the dataset consists of functions\n",
    "that are sampled anew from a GP at each iteration. The main difference between\n",
    "train and test in this case is that the test set contains more targets so that\n",
    "we can plot the entire curve, whereas the training set only contains a few\n",
    "target points to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvpjRr7SThlU"
   },
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "dataset_train = GPCurvesReader(\n",
    "    batch_size=64, max_num_context=MAX_CONTEXT_POINTS)\n",
    "data_train = dataset_train.generate_curves()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(\n",
    "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True)\n",
    "data_test = dataset_test.generate_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1R7TMkbqT586"
   },
   "source": [
    "We can now add the model to the graph and finalise it by defining the train step\n",
    "and the initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwcdCxsJUHS8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackdaw\\AppData\\Local\\Temp\\ipykernel_4688\\3697169770.py:41: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.compat.v1.layers.dense(hidden, size, name=\"Encoder_layer_{}\".format(i)))\n",
      "C:\\Users\\Jackdaw\\AppData\\Local\\Temp\\ipykernel_4688\\3697169770.py:44: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  hidden = tf.compat.v1.layers.dense(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`tape` is required when a `Tensor` loss is passed. Received: loss=1.7979029417037964, tape=None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Set up the optimizer and train step\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m train_step \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m init \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39minitialize_all_variables()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\optimizers\\optimizer.py:542\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, var_list, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    522\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m      None\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\optimizers\\optimizer.py:261\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute gradients of loss on trainable variables.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m  gradient can be `None`.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(loss) \u001b[38;5;129;01mand\u001b[39;00m tape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tape` is required when a `Tensor` loss is passed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     tape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGradientTape()\n",
      "\u001b[1;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=1.7979029417037964, tape=None."
     ]
    }
   ],
   "source": [
    "# Sizes of the layers of the MLPs for the encoder and decoder\n",
    "# The final output layer of the decoder outputs two values, one for the mean and\n",
    "# one for the variance of the prediction at the target location\n",
    "encoder_output_sizes = [128, 128, 128, 128]\n",
    "decoder_output_sizes = [128, 128, 2]\n",
    "\n",
    "# Define the model\n",
    "model = DeterministicModel(encoder_output_sizes, decoder_output_sizes)\n",
    "\n",
    "# Define the loss\n",
    "log_prob, _, _ = model(data_train.query, data_train.num_total_points,\n",
    "                       data_train.num_context_points, data_train.target_y)\n",
    "loss = -tf.reduce_mean(log_prob)\n",
    "\n",
    "# Get the predicted mean and variance at the target points for the testing set\n",
    "_, mu, sigma = model(data_test.query, data_test.num_total_points,\n",
    "                     data_test.num_context_points)\n",
    "\n",
    "# Set up the optimizer and train step\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "train_step = optimizer.minimize(loss , var_list=None )\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lf24kZWVVH_A"
   },
   "source": [
    "We are ready to train the model! During training we will plot some intermediate\n",
    "predictions to visualize how the model evolves.\n",
    "\n",
    "Every `PLOT_AFTER` iterations we print out the loss, which corresponds to the\n",
    "negative log probability of the ground truth targets under the predicted\n",
    "distribution. As the model is trained this value should decrease.\n",
    "\n",
    "In addition we are going to plot the predictions of our model alongside the\n",
    "ground truth curve and the context points that the CNP is provided at that\n",
    "iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "height": 2867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1796928,
     "status": "ok",
     "timestamp": 1537196774078,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "2Zrn1_lvVNRe",
    "outputId": "69bef975-ba2c-4c27-c9f8-69a966603d72"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9104/1891608314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINING_ITERATIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for it in range(TRAINING_ITERATIONS):\n",
    "    sess.run([train_step])\n",
    "\n",
    "    # Plot the predictions in `PLOT_AFTER` intervals\n",
    "    if it % PLOT_AFTER == 0:\n",
    "      loss_value, pred_y, var, target_y, whole_query = sess.run(\n",
    "          [loss, mu, sigma, data_test.target_y, data_test.query])\n",
    "\n",
    "      (context_x, context_y), target_x = whole_query\n",
    "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
    "\n",
    "      # Plot the prediction and the context\n",
    "      plot_functions(target_x, target_y, context_x, context_y, pred_y, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYFQODgm9eIp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Conditional Neural Processes OpenSource",
   "provenance": [
    {
     "file_id": "/piper/depot/google3/learning/deepmind/research/imaginative_agents/neural_processes/Conditional_Neural_Processes_OpenSource.ipynb?workspaceId=garnelo:opensource_NP::citc",
     "timestamp": 1533652807835
    },
    {
     "file_id": "1SYohd8cNXwJ360Kjcibu5DUmk6_z2KVT",
     "timestamp": 1533652159348
    },
    {
     "file_id": "1F-D4ElWS4UDjxs_1vuUF1G28B_2wWZjB",
     "timestamp": 1533297559549
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
